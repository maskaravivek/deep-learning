{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YOLO V1","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNyVVYsi4NwPlZ6NFY+zdSW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vcA95R45enBV","colab_type":"text"},"source":["# Implementing YOLOV1 from Scratch using Keras Tensorflow\n","\n","In this notebook I am going to implement YOLOV1 as described in the paper [You Only Look Once](https://arxiv.org/abs/1506.02640). The goal is to replicate the model as described in the paper and in the process, understand the nuances of using Keras on a complex problem. "]},{"cell_type":"code","metadata":{"id":"GkcrTpbpAvFj","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt    # for plotting the images\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0_h8kTGfbP4","colab_type":"text"},"source":["## Data Preprocessing\n","\n","I would be using [VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset as its size is manageable so it would be easy to run it using Google Colab. \n","\n","First, I download and extract the dataset. "]},{"cell_type":"code","metadata":{"id":"G2Lw77yYM1lF","colab_type":"code","colab":{}},"source":["!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n","\n","!tar xvf VOCtrainval_06-Nov-2007.tar\n","!tar xvf VOCtest_06-Nov-2007.tar\n","\n","!rm VOCtrainval_06-Nov-2007.tar\n","!rm VOCtest_06-Nov-2007.tar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKQz1Q9Nf0tZ","colab_type":"text"},"source":["Next, we process the annotations and write the labels in a text file. A text file is easier to consume as compared to XML. "]},{"cell_type":"code","metadata":{"id":"SQMm2Nm4PwqK","colab_type":"code","colab":{}},"source":["import argparse\n","import xml.etree.ElementTree as ET\n","import os\n","\n","parser = argparse.ArgumentParser(description='Build Annotations.')\n","parser.add_argument('dir', default='..', help='Annotations.')\n","\n","sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n","\n","classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n","               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n","               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n","               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","\n","\n","def convert_annotation(year, image_id, f):\n","    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n","    tree = ET.parse(in_file)\n","    root = tree.getroot()\n","\n","    for obj in root.iter('object'):\n","        difficult = obj.find('difficult').text\n","        cls = obj.find('name').text\n","        classes = list(classes_num.keys())\n","        if cls not in classes or int(difficult) == 1:\n","            continue\n","        cls_id = classes.index(cls)\n","        xmlbox = obj.find('bndbox')\n","        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n","             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n","        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEStgd47P3tQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592811478974,"user_tz":420,"elapsed":1475,"user":{"displayName":"vivek maskara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilKZNm10HGvDTdHN8Ivci6nDBOxQB9ycsvhbVQffQ=s64","userId":"09560284188560880275"}},"outputId":"29329caa-e458-4621-8d76-3b9c89840072"},"source":["for year, image_set in sets:\n","  print(year, image_set)\n","  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f:\n","      image_ids = f.read().strip().split()\n","  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n","      for image_id in image_ids:\n","          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n","          convert_annotation(year, image_id, f)\n","          f.write('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2007 train\n","2007 val\n","2007 test\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Igue5gbjyym5","colab_type":"text"},"source":["Next, I am adding a function to prepare the input and the output. The input is a (448, 448, 3) image and the output is a (7, 7, 30) tensor. The output is based on S x S x (B * 5 +C). \n","\n","S X S is the number of grids\n","B is the number of bounding boxes per grid\n","C is the number of predictions per grid"]},{"cell_type":"code","metadata":{"id":"pB1jqPlohjT2","colab_type":"code","colab":{}},"source":["import cv2 as cv\n","import numpy as np\n","\n","def read(image_path, label):\n","    image = cv.imread(image_path)\n","    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","    image_h, image_w = image.shape[0:2]\n","    image = cv.resize(image, (448, 448))\n","    image = image / 255.\n","\n","    label_matrix = np.zeros([7, 7, 30])\n","    for l in label:\n","        l = l.split(',')\n","        l = np.array(l, dtype=np.int)\n","        xmin = l[0]\n","        ymin = l[1]\n","        xmax = l[2]\n","        ymax = l[3]\n","        cls = l[4]\n","        x = (xmin + xmax) / 2 / image_w\n","        y = (ymin + ymax) / 2 / image_h\n","        w = (xmax - xmin) / image_w\n","        h = (ymax - ymin) / image_h\n","        loc = [7 * x, 7 * y]\n","        loc_i = int(loc[1])\n","        loc_j = int(loc[0])\n","        y = loc[1] - loc_i\n","        x = loc[0] - loc_j\n","\n","        if label_matrix[loc_i, loc_j, 24] == 0:\n","            label_matrix[loc_i, loc_j, cls] = 1\n","            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n","            label_matrix[loc_i, loc_j, 24] = 1  # response\n","\n","    return image, label_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grP6sQWA3Shp","colab_type":"text"},"source":["## Training the model\n","\n","Next, I am defining a custom generator that returns a batch of input and outputs. "]},{"cell_type":"code","metadata":{"id":"4OxTuZT23-c0","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","\n","class My_Custom_Generator(keras.utils.Sequence):\n","  \n","  def __init__(self, images, labels, batch_size):\n","    self.images = images\n","    self.labels = labels\n","    self.batch_size = batch_size\n","    \n","    \n","  def __len__(self) :\n","    return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n","  \n","  \n","  def __getitem__(self, idx) :\n","    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n","    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n","\n","    train_image = []\n","    train_label = []\n","\n","    for i in range(0, len(batch_x)):\n","      img_path = batch_x[i]\n","      label = batch_y[i]\n","      image, label_matrix = read(img_path, label)\n","      train_image.append(image)\n","      train_label.append(label_matrix)\n","    return np.array(train_image), np.array(train_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qm15e-T_3tOa","colab_type":"text"},"source":["The code snippet below, prepares arrays with inputs and outputs. "]},{"cell_type":"code","metadata":{"id":"Fe7ElYo960wW","colab_type":"code","colab":{}},"source":["train_datasets = []\n","val_datasets = []\n","\n","with open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n","    train_datasets = train_datasets + f.readlines()\n","with open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n","    val_datasets = val_datasets + f.readlines()\n","\n","X_train = []\n","Y_train = []\n","\n","X_val = []\n","Y_val = []\n","\n","for item in train_datasets:\n","  item = item.replace(\"\\n\", \"\").split(\" \")\n","  X_train.append(item[0])\n","  arr = []\n","  for i in range(1, len(item)):\n","    arr.append(item[i])\n","  Y_train.append(arr)\n","\n","for item in val_datasets:\n","  item = item.replace(\"\\n\", \"\").split(\" \")\n","  X_val.append(item[0])\n","  arr = []\n","  for i in range(1, len(item)):\n","    arr.append(item[i])\n","  Y_val.append(arr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jF4U-gbT37YM","colab_type":"text"},"source":["Next, we create instances of the generator for our training and validation sets. "]},{"cell_type":"code","metadata":{"id":"QNt_YsQLXthS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592811490703,"user_tz":420,"elapsed":628,"user":{"displayName":"vivek maskara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilKZNm10HGvDTdHN8Ivci6nDBOxQB9ycsvhbVQffQ=s64","userId":"09560284188560880275"}},"outputId":"2b3604cc-8cdd-4a19-9666-68b69b345b03"},"source":["batch_size = 4\n","my_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n","\n","my_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n","\n","x_train, y_train = my_training_batch_generator.__getitem__(0)\n","x_val, y_val = my_training_batch_generator.__getitem__(0)\n","print(x_train.shape)\n","print(y_train.shape)\n","\n","print(x_val.shape)\n","print(y_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4, 448, 448, 3)\n","(4, 7, 7, 30)\n","(4, 448, 448, 3)\n","(4, 7, 7, 30)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sy_3I6Hn3-ER","colab_type":"text"},"source":["### Define a custom output layer\n","\n","We need to reshape the output from the model so we define a custom Keras layer for it. "]},{"cell_type":"code","metadata":{"id":"2U5k_0_1QzoI","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import keras.backend as K\n","\n","class Yolo_Reshape(tf.keras.layers.Layer):\n","  def __init__(self, target_shape):\n","    super(Yolo_Reshape, self).__init__()\n","    self.target_shape = tuple(target_shape)\n","\n","  def get_config(self):\n","    config = super().get_config().copy()\n","    config.update({\n","        'target_shape': self.target_shape\n","    })\n","    return config\n","\n","  def call(self, input):\n","    # grids 7x7\n","    S = [self.target_shape[0], self.target_shape[1]]\n","    # classes\n","    C = 20\n","    # no of bounding boxes per grid\n","    B = 2\n","\n","    idx1 = S[0] * S[1] * C\n","    idx2 = idx1 + S[0] * S[1] * B\n","    \n","    # class probabilities\n","    class_probs = K.reshape(input[:, :idx1], (K.shape(input)[0],) + tuple([S[0], S[1], C]))\n","    class_probs = K.softmax(class_probs)\n","\n","    #confidence\n","    confs = K.reshape(input[:, idx1:idx2], (K.shape(input)[0],) + tuple([S[0], S[1], B]))\n","    confs = K.sigmoid(confs)\n","\n","    # boxes\n","    boxes = K.reshape(input[:, idx2:], (K.shape(input)[0],) + tuple([S[0], S[1], B * 4]))\n","    boxes = K.sigmoid(boxes)\n","\n","    outputs = K.concatenate([class_probs, confs, boxes])\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVOffcHE4Nif","colab_type":"text"},"source":["### Defining the YOLO model. \n","\n","Next, we define the model as described in the original paper. "]},{"cell_type":"code","metadata":{"id":"Z0eGd9X_7ekO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592811534848,"user_tz":420,"elapsed":851,"user":{"displayName":"vivek maskara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilKZNm10HGvDTdHN8Ivci6nDBOxQB9ycsvhbVQffQ=s64","userId":"09560284188560880275"}},"outputId":"90e41a2b-ff19-4000-dc9e-ce116c364544"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape, LeakyReLU\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n","from tensorflow.keras.regularizers import l2\n","\n","lrelu = LeakyReLU(alpha=0.1)\n","\n","nb_boxes=1\n","grid_w=7\n","grid_h=7\n","cell_w=64\n","cell_h=64\n","img_w=grid_w*cell_w\n","img_h=grid_h*cell_h\n","\n","model = Sequential()\n","model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), input_shape =(img_h, img_w, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu))\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Dense(1024))\n","model.add(Dropout(0.5))\n","model.add(Dense(1470, activation='sigmoid'))\n","model.add(Yolo_Reshape(target_shape=(7,7,30)))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_72 (Conv2D)           (None, 448, 448, 64)      9472      \n","_________________________________________________________________\n","max_pooling2d_12 (MaxPooling (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","conv2d_73 (Conv2D)           (None, 224, 224, 192)     110784    \n","_________________________________________________________________\n","max_pooling2d_13 (MaxPooling (None, 112, 112, 192)     0         \n","_________________________________________________________________\n","conv2d_74 (Conv2D)           (None, 112, 112, 128)     24704     \n","_________________________________________________________________\n","conv2d_75 (Conv2D)           (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","conv2d_76 (Conv2D)           (None, 112, 112, 256)     65792     \n","_________________________________________________________________\n","conv2d_77 (Conv2D)           (None, 112, 112, 512)     1180160   \n","_________________________________________________________________\n","max_pooling2d_14 (MaxPooling (None, 56, 56, 512)       0         \n","_________________________________________________________________\n","conv2d_78 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_79 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_80 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_81 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_82 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_83 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_84 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_85 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_86 (Conv2D)           (None, 56, 56, 512)       262656    \n","_________________________________________________________________\n","conv2d_87 (Conv2D)           (None, 56, 56, 1024)      4719616   \n","_________________________________________________________________\n","max_pooling2d_15 (MaxPooling (None, 28, 28, 1024)      0         \n","_________________________________________________________________\n","conv2d_88 (Conv2D)           (None, 28, 28, 512)       524800    \n","_________________________________________________________________\n","conv2d_89 (Conv2D)           (None, 28, 28, 1024)      4719616   \n","_________________________________________________________________\n","conv2d_90 (Conv2D)           (None, 28, 28, 512)       524800    \n","_________________________________________________________________\n","conv2d_91 (Conv2D)           (None, 28, 28, 1024)      4719616   \n","_________________________________________________________________\n","conv2d_92 (Conv2D)           (None, 28, 28, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_93 (Conv2D)           (None, 14, 14, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_94 (Conv2D)           (None, 12, 12, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_95 (Conv2D)           (None, 10, 10, 1024)      9438208   \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 102400)            0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 512)               52429312  \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1024)              525312    \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 1470)              1506750   \n","_________________________________________________________________\n","yolo__reshape_2 (Yolo_Reshap (None, 7, 7, 30)          0         \n","=================================================================\n","Total params: 114,617,342\n","Trainable params: 114,617,342\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i9SGPNmS4qGu","colab_type":"text"},"source":["### Define a custom learning rate scheduler\n","\n","The paper uses different learning rates for different epochs. So we define a custom Callback function for the learning rate. "]},{"cell_type":"code","metadata":{"id":"XLvhKEb5GKhI","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","\n","class CustomLearningRateScheduler(keras.callbacks.Callback):\n","    def __init__(self, schedule):\n","        super(CustomLearningRateScheduler, self).__init__()\n","        self.schedule = schedule\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        if not hasattr(self.model.optimizer, \"lr\"):\n","            raise ValueError('Optimizer must have a \"lr\" attribute.')\n","        # Get the current learning rate from model's optimizer.\n","        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n","        # Call schedule function to get the scheduled learning rate.\n","        scheduled_lr = self.schedule(epoch, lr)\n","        # Set the value back to the optimizer before this epoch starts\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n","        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n","\n","\n","LR_SCHEDULE = [\n","    # (epoch to start, learning rate) tuples\n","    (0, 0.01),\n","    (75, 0.001),\n","    (105, 0.0001),\n","]\n","\n","\n","def lr_schedule(epoch, lr):\n","    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n","    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n","        return lr\n","    for i in range(len(LR_SCHEDULE)):\n","        if epoch == LR_SCHEDULE[i][0]:\n","            return LR_SCHEDULE[i][1]\n","    return lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9_hD2d445zw","colab_type":"text"},"source":["### Define the loss function\n","\n","Next, we would be defining a custom loss function to be used in the model. Take a look at this blog post to understand more about the [loss function used in YOLO](https://hackernoon.com/understanding-yolo-f5a74bbc7967). \n","\n","I understood the loss function but didn't implement it on my own. I took the implementation as it is from this [Github repo](https://github.com/JY-112553/yolov1-keras-voc)."]},{"cell_type":"code","metadata":{"id":"MN-oFCQolCUi","colab_type":"code","colab":{}},"source":["import keras.backend as K\n","\n","\n","def xywh2minmax(xy, wh):\n","    xy_min = xy - wh / 2\n","    xy_max = xy + wh / 2\n","\n","    return xy_min, xy_max\n","\n","\n","def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n","    intersect_mins = K.maximum(pred_mins, true_mins)\n","    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n","    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n","    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n","\n","    pred_wh = pred_maxes - pred_mins\n","    true_wh = true_maxes - true_mins\n","    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n","    true_areas = true_wh[..., 0] * true_wh[..., 1]\n","\n","    union_areas = pred_areas + true_areas - intersect_areas\n","    iou_scores = intersect_areas / union_areas\n","\n","    return iou_scores\n","\n","\n","def yolo_head(feats):\n","    # Dynamic implementation of conv dims for fully convolutional model.\n","    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n","    # In YOLO the height index is the inner most iteration.\n","    conv_height_index = K.arange(0, stop=conv_dims[0])\n","    conv_width_index = K.arange(0, stop=conv_dims[1])\n","    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n","\n","    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n","    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n","    conv_width_index = K.tile(\n","        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n","    conv_width_index = K.flatten(K.transpose(conv_width_index))\n","    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n","    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n","    conv_index = K.cast(conv_index, K.dtype(feats))\n","\n","    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n","\n","    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n","    box_wh = feats[..., 2:4] * 448\n","\n","    return box_xy, box_wh\n","\n","\n","def yolo_loss(y_true, y_pred):\n","    label_class = y_true[..., :20]  # ? * 7 * 7 * 20\n","    label_box = y_true[..., 20:24]  # ? * 7 * 7 * 4\n","    response_mask = y_true[..., 24]  # ? * 7 * 7\n","    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n","\n","    predict_class = y_pred[..., :20]  # ? * 7 * 7 * 20\n","    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n","    predict_box = y_pred[..., 22:]  # ? * 7 * 7 * 8\n","\n","    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n","    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n","\n","    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n","    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n","    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n","    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n","\n","    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n","    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n","    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n","    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n","\n","    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n","    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n","    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n","\n","    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n","\n","    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n","    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n","    confidence_loss = no_object_loss + object_loss\n","    confidence_loss = K.sum(confidence_loss)\n","\n","    class_loss = response_mask * K.square(label_class - predict_class)\n","    class_loss = K.sum(class_loss)\n","\n","    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n","    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n","\n","    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n","    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n","\n","    box_mask = K.expand_dims(box_mask)\n","    response_mask = K.expand_dims(response_mask)\n","\n","    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n","    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n","    box_loss = K.sum(box_loss)\n","\n","    loss = confidence_loss + class_loss + box_loss\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgSKBlj35e5_","colab_type":"text"},"source":["### Add a callback for saving the weights\n","\n","Next, I define a callback to keep saving the best weights. "]},{"cell_type":"code","metadata":{"id":"w4Fs-f9eZXBO","colab_type":"code","colab":{}},"source":["# defining a function to save the weights of best model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlwmFdWy5r1h","colab_type":"text"},"source":["### Compile the model\n","\n","Finally, I compile the model using the custom loss function that was defined above. "]},{"cell_type":"code","metadata":{"id":"sDOQ5IvFLxDD","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","\n","model.compile(loss=yolo_loss ,optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlPF3nyX5yNu","colab_type":"text"},"source":["### Train the model\n","\n","Now that we have everything setup, we will call `model.fit` to train the model for 135 epochs. "]},{"cell_type":"code","metadata":{"id":"9qqi3Vt_LzbC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592837241401,"user_tz":420,"elapsed":25696910,"user":{"displayName":"vivek maskara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilKZNm10HGvDTdHN8Ivci6nDBOxQB9ycsvhbVQffQ=s64","userId":"09560284188560880275"}},"outputId":"67f83c4a-a732-4794-c93b-92ee4a34aa96"},"source":["model.fit(x=my_training_batch_generator,\n","          steps_per_epoch = int(len(X_train) // batch_size),\n","          epochs = 135,\n","          verbose = 1,\n","          workers= 4,\n","          validation_data = my_validation_batch_generator,\n","          validation_steps = int(len(X_val) // batch_size),\n","           callbacks=[\n","              CustomLearningRateScheduler(lr_schedule),\n","              mcp_save\n","          ])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch 00000: Learning rate is 0.0100.\n","Epoch 1/135\n","625/625 [==============================] - 196s 314ms/step - loss: 72.2096 - val_loss: 65.7879\n","\n","Epoch 00001: Learning rate is 0.0100.\n","Epoch 2/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3456 - val_loss: 65.7879\n","\n","Epoch 00002: Learning rate is 0.0100.\n","Epoch 3/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9994 - val_loss: 65.7879\n","\n","Epoch 00003: Learning rate is 0.0100.\n","Epoch 4/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2797 - val_loss: 65.7879\n","\n","Epoch 00004: Learning rate is 0.0100.\n","Epoch 5/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2525 - val_loss: 65.7879\n","\n","Epoch 00005: Learning rate is 0.0100.\n","Epoch 6/135\n","625/625 [==============================] - 190s 305ms/step - loss: 72.2210 - val_loss: 65.7879\n","\n","Epoch 00006: Learning rate is 0.0100.\n","Epoch 7/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0765 - val_loss: 65.7879\n","\n","Epoch 00007: Learning rate is 0.0100.\n","Epoch 8/135\n","625/625 [==============================] - 190s 305ms/step - loss: 72.0917 - val_loss: 65.7879\n","\n","Epoch 00008: Learning rate is 0.0100.\n","Epoch 9/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2387 - val_loss: 65.7879\n","\n","Epoch 00009: Learning rate is 0.0100.\n","Epoch 10/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8363 - val_loss: 65.7879\n","\n","Epoch 00010: Learning rate is 0.0100.\n","Epoch 11/135\n","625/625 [==============================] - 190s 305ms/step - loss: 72.1311 - val_loss: 65.7879\n","\n","Epoch 00011: Learning rate is 0.0100.\n","Epoch 12/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1031 - val_loss: 65.7879\n","\n","Epoch 00012: Learning rate is 0.0100.\n","Epoch 13/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0708 - val_loss: 65.7879\n","\n","Epoch 00013: Learning rate is 0.0100.\n","Epoch 14/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3605 - val_loss: 65.7879\n","\n","Epoch 00014: Learning rate is 0.0100.\n","Epoch 15/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.5191 - val_loss: 65.7879\n","\n","Epoch 00015: Learning rate is 0.0100.\n","Epoch 16/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9991 - val_loss: 65.7879\n","\n","Epoch 00016: Learning rate is 0.0100.\n","Epoch 17/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2296 - val_loss: 65.7879\n","\n","Epoch 00017: Learning rate is 0.0100.\n","Epoch 18/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3003 - val_loss: 65.7879\n","\n","Epoch 00018: Learning rate is 0.0100.\n","Epoch 19/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3860 - val_loss: 65.7879\n","\n","Epoch 00019: Learning rate is 0.0100.\n","Epoch 20/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.4065 - val_loss: 65.7879\n","\n","Epoch 00020: Learning rate is 0.0100.\n","Epoch 21/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0564 - val_loss: 65.7879\n","\n","Epoch 00021: Learning rate is 0.0100.\n","Epoch 22/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2607 - val_loss: 65.7879\n","\n","Epoch 00022: Learning rate is 0.0100.\n","Epoch 23/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1305 - val_loss: 65.7879\n","\n","Epoch 00023: Learning rate is 0.0100.\n","Epoch 24/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8438 - val_loss: 65.7879\n","\n","Epoch 00024: Learning rate is 0.0100.\n","Epoch 25/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1438 - val_loss: 65.7879\n","\n","Epoch 00025: Learning rate is 0.0100.\n","Epoch 26/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2802 - val_loss: 65.7879\n","\n","Epoch 00026: Learning rate is 0.0100.\n","Epoch 27/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.7967 - val_loss: 65.7879\n","\n","Epoch 00027: Learning rate is 0.0100.\n","Epoch 28/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.4962 - val_loss: 65.7879\n","\n","Epoch 00028: Learning rate is 0.0100.\n","Epoch 29/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3844 - val_loss: 65.7879\n","\n","Epoch 00029: Learning rate is 0.0100.\n","Epoch 30/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0479 - val_loss: 65.7879\n","\n","Epoch 00030: Learning rate is 0.0100.\n","Epoch 31/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0611 - val_loss: 65.7879\n","\n","Epoch 00031: Learning rate is 0.0100.\n","Epoch 32/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2179 - val_loss: 65.7879\n","\n","Epoch 00032: Learning rate is 0.0100.\n","Epoch 33/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2435 - val_loss: 65.7879\n","\n","Epoch 00033: Learning rate is 0.0100.\n","Epoch 34/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1711 - val_loss: 65.7879\n","\n","Epoch 00034: Learning rate is 0.0100.\n","Epoch 35/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.2095 - val_loss: 65.7879\n","\n","Epoch 00035: Learning rate is 0.0100.\n","Epoch 36/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.1052 - val_loss: 65.7879\n","\n","Epoch 00036: Learning rate is 0.0100.\n","Epoch 37/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0387 - val_loss: 65.7879\n","\n","Epoch 00037: Learning rate is 0.0100.\n","Epoch 38/135\n","625/625 [==============================] - 190s 303ms/step - loss: 71.9203 - val_loss: 65.7879\n","\n","Epoch 00038: Learning rate is 0.0100.\n","Epoch 39/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.0965 - val_loss: 65.7879\n","\n","Epoch 00039: Learning rate is 0.0100.\n","Epoch 40/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9976 - val_loss: 65.7879\n","\n","Epoch 00040: Learning rate is 0.0100.\n","Epoch 41/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1823 - val_loss: 65.7879\n","\n","Epoch 00041: Learning rate is 0.0100.\n","Epoch 42/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3243 - val_loss: 65.7879\n","\n","Epoch 00042: Learning rate is 0.0100.\n","Epoch 43/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.1714 - val_loss: 65.7879\n","\n","Epoch 00043: Learning rate is 0.0100.\n","Epoch 44/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9977 - val_loss: 65.7879\n","\n","Epoch 00044: Learning rate is 0.0100.\n","Epoch 45/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2731 - val_loss: 65.7879\n","\n","Epoch 00045: Learning rate is 0.0100.\n","Epoch 46/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1929 - val_loss: 65.7879\n","\n","Epoch 00046: Learning rate is 0.0100.\n","Epoch 47/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3631 - val_loss: 65.7879\n","\n","Epoch 00047: Learning rate is 0.0100.\n","Epoch 48/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.3618 - val_loss: 65.7879\n","\n","Epoch 00048: Learning rate is 0.0100.\n","Epoch 49/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0511 - val_loss: 65.7879\n","\n","Epoch 00049: Learning rate is 0.0100.\n","Epoch 50/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2566 - val_loss: 65.7879\n","\n","Epoch 00050: Learning rate is 0.0100.\n","Epoch 51/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3441 - val_loss: 65.7879\n","\n","Epoch 00051: Learning rate is 0.0100.\n","Epoch 52/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1696 - val_loss: 65.7879\n","\n","Epoch 00052: Learning rate is 0.0100.\n","Epoch 53/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0938 - val_loss: 65.7879\n","\n","Epoch 00053: Learning rate is 0.0100.\n","Epoch 54/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0704 - val_loss: 65.7879\n","\n","Epoch 00054: Learning rate is 0.0100.\n","Epoch 55/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9594 - val_loss: 65.7879\n","\n","Epoch 00055: Learning rate is 0.0100.\n","Epoch 56/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0935 - val_loss: 65.7879\n","\n","Epoch 00056: Learning rate is 0.0100.\n","Epoch 57/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9528 - val_loss: 65.7879\n","\n","Epoch 00057: Learning rate is 0.0100.\n","Epoch 58/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0921 - val_loss: 65.7879\n","\n","Epoch 00058: Learning rate is 0.0100.\n","Epoch 59/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0623 - val_loss: 65.7879\n","\n","Epoch 00059: Learning rate is 0.0100.\n","Epoch 60/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1637 - val_loss: 65.7879\n","\n","Epoch 00060: Learning rate is 0.0100.\n","Epoch 61/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0498 - val_loss: 65.7879\n","\n","Epoch 00061: Learning rate is 0.0100.\n","Epoch 62/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1009 - val_loss: 65.7879\n","\n","Epoch 00062: Learning rate is 0.0100.\n","Epoch 63/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0010 - val_loss: 65.7879\n","\n","Epoch 00063: Learning rate is 0.0100.\n","Epoch 64/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2677 - val_loss: 65.7879\n","\n","Epoch 00064: Learning rate is 0.0100.\n","Epoch 65/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3168 - val_loss: 65.7879\n","\n","Epoch 00065: Learning rate is 0.0100.\n","Epoch 66/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3855 - val_loss: 65.7879\n","\n","Epoch 00066: Learning rate is 0.0100.\n","Epoch 67/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1543 - val_loss: 65.7879\n","\n","Epoch 00067: Learning rate is 0.0100.\n","Epoch 68/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0378 - val_loss: 65.7879\n","\n","Epoch 00068: Learning rate is 0.0100.\n","Epoch 69/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.1595 - val_loss: 65.7879\n","\n","Epoch 00069: Learning rate is 0.0100.\n","Epoch 70/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1202 - val_loss: 65.7879\n","\n","Epoch 00070: Learning rate is 0.0100.\n","Epoch 71/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9380 - val_loss: 65.7879\n","\n","Epoch 00071: Learning rate is 0.0100.\n","Epoch 72/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9040 - val_loss: 65.7879\n","\n","Epoch 00072: Learning rate is 0.0100.\n","Epoch 73/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3407 - val_loss: 65.7879\n","\n","Epoch 00073: Learning rate is 0.0100.\n","Epoch 74/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2056 - val_loss: 65.7879\n","\n","Epoch 00074: Learning rate is 0.0100.\n","Epoch 75/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8586 - val_loss: 65.7879\n","\n","Epoch 00075: Learning rate is 0.0010.\n","Epoch 76/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0458 - val_loss: 65.7879\n","\n","Epoch 00076: Learning rate is 0.0010.\n","Epoch 77/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.7863 - val_loss: 65.7879\n","\n","Epoch 00077: Learning rate is 0.0010.\n","Epoch 78/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9642 - val_loss: 65.7879\n","\n","Epoch 00078: Learning rate is 0.0010.\n","Epoch 79/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2432 - val_loss: 65.7879\n","\n","Epoch 00079: Learning rate is 0.0010.\n","Epoch 80/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2702 - val_loss: 65.7879\n","\n","Epoch 00080: Learning rate is 0.0010.\n","Epoch 81/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.5619 - val_loss: 65.7879\n","\n","Epoch 00081: Learning rate is 0.0010.\n","Epoch 82/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2247 - val_loss: 65.7879\n","\n","Epoch 00082: Learning rate is 0.0010.\n","Epoch 83/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3948 - val_loss: 65.7879\n","\n","Epoch 00083: Learning rate is 0.0010.\n","Epoch 84/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3117 - val_loss: 65.7879\n","\n","Epoch 00084: Learning rate is 0.0010.\n","Epoch 85/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9370 - val_loss: 65.7879\n","\n","Epoch 00085: Learning rate is 0.0010.\n","Epoch 86/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.5628 - val_loss: 65.7879\n","\n","Epoch 00086: Learning rate is 0.0010.\n","Epoch 87/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0737 - val_loss: 65.7879\n","\n","Epoch 00087: Learning rate is 0.0010.\n","Epoch 88/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3263 - val_loss: 65.7879\n","\n","Epoch 00088: Learning rate is 0.0010.\n","Epoch 89/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2957 - val_loss: 65.7879\n","\n","Epoch 00089: Learning rate is 0.0010.\n","Epoch 90/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0724 - val_loss: 65.7879\n","\n","Epoch 00090: Learning rate is 0.0010.\n","Epoch 91/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2740 - val_loss: 65.7879\n","\n","Epoch 00091: Learning rate is 0.0010.\n","Epoch 92/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3272 - val_loss: 65.7879\n","\n","Epoch 00092: Learning rate is 0.0010.\n","Epoch 93/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0887 - val_loss: 65.7879\n","\n","Epoch 00093: Learning rate is 0.0010.\n","Epoch 94/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0960 - val_loss: 65.7879\n","\n","Epoch 00094: Learning rate is 0.0010.\n","Epoch 95/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2268 - val_loss: 65.7879\n","\n","Epoch 00095: Learning rate is 0.0010.\n","Epoch 96/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2024 - val_loss: 65.7879\n","\n","Epoch 00096: Learning rate is 0.0010.\n","Epoch 97/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8824 - val_loss: 65.7879\n","\n","Epoch 00097: Learning rate is 0.0010.\n","Epoch 98/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9064 - val_loss: 65.7879\n","\n","Epoch 00098: Learning rate is 0.0010.\n","Epoch 99/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8161 - val_loss: 65.7879\n","\n","Epoch 00099: Learning rate is 0.0010.\n","Epoch 100/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2926 - val_loss: 65.7879\n","\n","Epoch 00100: Learning rate is 0.0010.\n","Epoch 101/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8214 - val_loss: 65.7879\n","\n","Epoch 00101: Learning rate is 0.0010.\n","Epoch 102/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.1726 - val_loss: 65.7879\n","\n","Epoch 00102: Learning rate is 0.0010.\n","Epoch 103/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9306 - val_loss: 65.7879\n","\n","Epoch 00103: Learning rate is 0.0010.\n","Epoch 104/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9705 - val_loss: 65.7879\n","\n","Epoch 00104: Learning rate is 0.0010.\n","Epoch 105/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.2382 - val_loss: 65.7879\n","\n","Epoch 00105: Learning rate is 0.0001.\n","Epoch 106/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2247 - val_loss: 65.7879\n","\n","Epoch 00106: Learning rate is 0.0001.\n","Epoch 107/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1260 - val_loss: 65.7879\n","\n","Epoch 00107: Learning rate is 0.0001.\n","Epoch 108/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0190 - val_loss: 65.7879\n","\n","Epoch 00108: Learning rate is 0.0001.\n","Epoch 109/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3824 - val_loss: 65.7879\n","\n","Epoch 00109: Learning rate is 0.0001.\n","Epoch 110/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9743 - val_loss: 65.7879\n","\n","Epoch 00110: Learning rate is 0.0001.\n","Epoch 111/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2580 - val_loss: 65.7879\n","\n","Epoch 00111: Learning rate is 0.0001.\n","Epoch 112/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9747 - val_loss: 65.7879\n","\n","Epoch 00112: Learning rate is 0.0001.\n","Epoch 113/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.2303 - val_loss: 65.7879\n","\n","Epoch 00113: Learning rate is 0.0001.\n","Epoch 114/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1642 - val_loss: 65.7879\n","\n","Epoch 00114: Learning rate is 0.0001.\n","Epoch 115/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.4621 - val_loss: 65.7879\n","\n","Epoch 00115: Learning rate is 0.0001.\n","Epoch 116/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0294 - val_loss: 65.7879\n","\n","Epoch 00116: Learning rate is 0.0001.\n","Epoch 117/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9352 - val_loss: 65.7879\n","\n","Epoch 00117: Learning rate is 0.0001.\n","Epoch 118/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3180 - val_loss: 65.7879\n","\n","Epoch 00118: Learning rate is 0.0001.\n","Epoch 119/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3319 - val_loss: 65.7879\n","\n","Epoch 00119: Learning rate is 0.0001.\n","Epoch 120/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1282 - val_loss: 65.7879\n","\n","Epoch 00120: Learning rate is 0.0001.\n","Epoch 121/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1484 - val_loss: 65.7879\n","\n","Epoch 00121: Learning rate is 0.0001.\n","Epoch 122/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.0230 - val_loss: 65.7879\n","\n","Epoch 00122: Learning rate is 0.0001.\n","Epoch 123/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2825 - val_loss: 65.7879\n","\n","Epoch 00123: Learning rate is 0.0001.\n","Epoch 124/135\n","625/625 [==============================] - 190s 305ms/step - loss: 72.3125 - val_loss: 65.7879\n","\n","Epoch 00124: Learning rate is 0.0001.\n","Epoch 125/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2313 - val_loss: 65.7879\n","\n","Epoch 00125: Learning rate is 0.0001.\n","Epoch 126/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.0643 - val_loss: 65.7879\n","\n","Epoch 00126: Learning rate is 0.0001.\n","Epoch 127/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.3512 - val_loss: 65.7879\n","\n","Epoch 00127: Learning rate is 0.0001.\n","Epoch 128/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.1472 - val_loss: 65.7879\n","\n","Epoch 00128: Learning rate is 0.0001.\n","Epoch 129/135\n","625/625 [==============================] - 190s 303ms/step - loss: 72.2152 - val_loss: 65.7879\n","\n","Epoch 00129: Learning rate is 0.0001.\n","Epoch 130/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.9963 - val_loss: 65.7879\n","\n","Epoch 00130: Learning rate is 0.0001.\n","Epoch 131/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1563 - val_loss: 65.7879\n","\n","Epoch 00131: Learning rate is 0.0001.\n","Epoch 132/135\n","625/625 [==============================] - 190s 304ms/step - loss: 71.8043 - val_loss: 65.7879\n","\n","Epoch 00132: Learning rate is 0.0001.\n","Epoch 133/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.2035 - val_loss: 65.7879\n","\n","Epoch 00133: Learning rate is 0.0001.\n","Epoch 134/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.1887 - val_loss: 65.7879\n","\n","Epoch 00134: Learning rate is 0.0001.\n","Epoch 135/135\n","625/625 [==============================] - 190s 304ms/step - loss: 72.3559 - val_loss: 65.7879\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbc44753eb8>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"rxmdBXaE59az","colab_type":"text"},"source":["## Conclusion\n","\n","It was a good exercise to implement YOLO V1 from scratch and understand various nuances of writing a model from scratch. This implementation won't achieve the same accuracy as what was described in the paper since we have skipped the pretraining step. "]}]}